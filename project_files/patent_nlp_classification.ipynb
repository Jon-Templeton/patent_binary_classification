{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent Binary Classification: *NLP*\n",
    "## By Jon Templeton\n",
    "\n",
    "In this notebook I will be creating a binary classification of patents using Natural Language Processing. I am using the pretrained BERT model `distilbert-base-uncased` from Hugging Face. For the patent dataset, I will be using a combination of the title and abstract columns to train the model.\n",
    "\n",
    "Steps:\n",
    "1. Data Loading & Cleaning\n",
    "2. Downsample Data\n",
    "3. Data Preparation & Tokenization\n",
    "4. Evaluation Metrics Setup\n",
    "5. Model Initialization & Training Configuration\n",
    "6. Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Cleaning\n",
    "\n",
    "Load the dataset and perform initial cleaning steps. This includes removing duplicate rows, removing unnecessary columns, and handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from parquet file\n",
    "df = pd.read_parquet(\"ml_dataset.parquet\")\n",
    "\n",
    "# Remove the duplicates so that we have only one row per patent\n",
    "df = df.drop_duplicates(subset=['title', 'abstract'], keep='first')\n",
    "\n",
    "# Modify data columns\n",
    "df['text'] = df['title'] + ' ' + df['abstract']\n",
    "df = df.drop(columns=['title', 'abstract', 'ucid', 'code', \"cpc_first_4\"])\n",
    "\n",
    "# Remove the rows with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample Data\n",
    "\n",
    "The dataset is imbalanced at a ratio greater than 1:700. For better performance, I downsampled the majority class to balance the training data. This helps improve model performance by reducing bias towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# There is an imbalance in the dataset\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df['labels'] == 0]\n",
    "df_minority = df[df['labels'] == 1]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,\n",
    "                                   n_samples=len(df_minority),  # match minority class\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "df_downsampled = df_downsampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Tokenization\n",
    "\n",
    "Prepare dataset for classification by performing a train-test split and tokenization using `AutoTokenizer` from the Hugging Face `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38852480c98641349ecdd82fd1af7351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/265 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14af5c457eb84eddb65102f17cf1b182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Determine the train and test datasets\n",
    "dataset = Dataset.from_pandas(df_downsampled)\n",
    "train_test_dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "tokenized_dataset = train_test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Setup\n",
    "\n",
    "Load evaluation metrics like accuracy, precision, recall, and F1 score using the Hugging Face `evaluate` library, and define a function `compute_metrics()` for computing these metrics on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load Evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization & Training Configuration\n",
    "\n",
    "Load pre-trained BERT model `'distilbert-base-uncased'`, and set up the training parameters. Then instantiate the `Trainer` with the model, training arguments, datasets, and the custom metrics computation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "Time to actually train the model and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97b8b59c3b74b57ae9b1df7930f89f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.084, 'train_samples_per_second': 31.693, 'train_steps_per_second': 2.033, 'train_loss': 0.3816037271537033, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0de61f7f31048a3927fb998be943821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy: 0.9253731343283582\n",
      "Precision: 0.9166666666666666\n",
      "Recall: 0.9428571428571428\n",
      "F1 Score: 0.9295774647887323\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Extract and print accuracy, precision, recall, and F1 score\n",
    "accuracy = results.get(\"eval_accuracy\")\n",
    "precision = results.get(\"eval_precision\")\n",
    "recall = results.get(\"eval_recall\")\n",
    "f1 = results.get(\"eval_f1\")\n",
    "\n",
    "print(f\"\\n\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./results/model02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has successfully implemented a binary classification model using NLP techniques. The model's training is quick and achieves reasonable results based on the performance metrics.\n",
    "\n",
    "In the other notebook `patent_linear_regression.ipynb`, I built a binary classifier using a linear regression model trained on just the CPC Codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
